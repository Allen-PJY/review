# 爬虫

### 说一下爬虫程序执行的流程（框架和三方库均可）


### 爬虫在向数据库存数据开始和结束都会发一条消息，是scrapy哪个模块实现的
- Item Pipeline scrapy的信号处理使用的是 dispatch模块

### 爬取下来的数据如何去重，说一下具体的算法依据
- 通过MD5生成电子指纹来判断页面是否改变
- `nutch` 去重。
    - nutch 中digest 是对采集的每一个网页内容的 32位哈希值，如果两个网页内容完全一样，它们的 digest 值肯定会 一样。

### 写爬虫是用`多进程`还是`多线程`更好 
- IO 密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率
- 单线程下有IO操作会进行IO 等待，造成不必要的时间浪费
- 而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU 的资源，从而能提升程序执行效率
- 在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程。

### 说一下numpy和pandas的区别 分别的应用场景
- `Numpy`是`Scipy`的扩展包，纯数学。
- `Pandas` 以矩阵为基础的数学计算模块。
    - 提供了一套名为DataFrame的数据结构，比较契合统计分析中的表结构
    - 提供了计算接口，可用`Numpy`或其它方式进行计算。

### 验证码如何处理
- Scrapy自带处理验证码
- 获取到验证码图片的url，调用打码平台处理验证码

### 微信公众号数据如何抓取
- sogou 微信搜索数据

### 动态的股票信息如何抓取

- 股票数据的获取目前有如下两种方法可以获取:
    - http/JavaScript 接口取数据
    - web-service 接口
    - Sina 股票数据接口

- 以贵州茅台（股票代码：600519）为例，如果要获取它的最新行情，只需访问新浪的股票数据
    - ``https://hq.sinajs.cn/list=sh600519``
    - 返回的数据：

    > ``var hq_str_sh600519="贵州茅台,738.000,741.970,736.410,743.560,730.000,736.100,736.590,2599190,1912613830.000,300,736.100,100,736.060,100,736.050,500,736.000,200,735.900,200,736.590,900,736.600,100,736.670,100,736.760,400,736.770,2018-03-02,15:00:00,00";``

### 爬虫部署
- 利用`scrapyd`进行爬虫部署

    > [scrapyd部署总结](http://blog.csdn.net/xiaoquantouer/article/details/53164306)

### scrapy去重

- 数据量不大时，可以直接放在内存里面进行去重，python可以使用set()进行去重
- 数据需要持久化时可以使用redis的set数据结构。
- 当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成16/32/40个字符，再使用上面两种方法去重；
- 当数据量达到亿（甚至十亿、百亿）数量级时，内存有限，必须用“位”来去重，才能够满足需求。

> 然而Bloomfilter运行在一台机器的内存上，不方便持久化（机器down掉就什么都没啦），也不方便分布式爬虫的统一去重。
Bloomfilter就是将去重对象映射到几个内存“位”，通过几个位的0/1值来判断一个对象是否已经存在。
如果可以在Redis上申请内存进行Bloomfilter，以上两个问题就都能解决了。

> simhash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字，
然后判断重复只需要判断他们的特征字的距离是不是小于n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。

### 分布式有哪些方案，哪一种最好
- `celery`、`beanstalk`，`gearman`

- 个人认为 `gearman` 比较好。原因主要有以下几点：
    - 1）技术类型简单，维护成本低。
    - 2）简单至上。能满足当前的技术需求即可 (分布式任务处理、异步同步任务同时支持、任务队列的持久化、维护部署简单)。
    - 3）有成熟的使用案例。instagram 就是使用的 gearman 来完成图片的处理的相关任务，有成功的经验，我们当然应该借鉴。



### 谈一谈你对Selenium和PhantomJS了解
- Selenium是一个Web的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发
生。Selenium自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们
可以用一个叫PhantomJS的工具代替真实的浏览器。
Selenium库里有个叫WebDriver的API。WebDriver有点儿像可以加载网站的浏览器，但是它也可以像XPath或者其他Selector对象一样用来查找页
面元素，与页面上的元素进行交互(发送文本、点击等)，以及执行其它动作来运行网络爬虫。

- PhantomJS是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的JavaScript，
因为不会展示图形界面，所以运行起来比完整的浏览器要高效。

- 如果我们把Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了，
这个爬虫可以处理JavaScript、Cookies、headers，以及任何我们真实用户需要做的事情。

### 常见的反爬虫和应对方法
- **1).通过Headers反爬虫**
    - 从用户请求的Headers反爬虫是最常见的反爬虫策略，在爬虫中修改或者添加Headers就能很好的绕过。
    - 很多网站都会对Headers的**User-Agent**进行检测 -> 将浏览器的User-Agent复制到爬虫的Headers中
    - 还有一部分网站会对**Refer**进行检测（一些资源网站的防盗链就是检测Refer)。 -> 将Refer值修改为目标网站域名

- **2).基于用户行为反爬虫**
    - **同一IP** 短时间内多次访问同一页面(大部分网站)
        - 使用IP代理
        - 专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。可以每请求几次更换一个ip
    - **同一账户** 短时间内多次进行相同操作
        - 可以在每次请求后随机间隔几秒再进行下一次请求
        - 有些有逻辑漏洞的网站，可以通过“请求-退出登录-重新登录-继续请求”，来绕过*同一账号短时间内不能多次进行相同请求*的限制。
- **3).动态页面的反爬虫**
    - 还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。
    - 首先用Fiddler对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests模拟ajax请求，对响应的json进行分析得到需要的数据。
    - 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。
    - 这种情况下就用selenium+phantomJS，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本

### 动态加载又对及时性要求很高怎么处理
- Selenium+Phantomjs
- 尽量不使用sleep而使用WebDriverWait

### 分布式爬虫主要解决什么问题
- ip
- 带宽
- cpu
- io


### python爬虫有哪些常用技术
- Scrapy
- requests
- XPath
- BeautifulSoup
- urllib
- urllib2

### 简单说一下你对scrapy的了解
scrapy是一个快速(fast)、高层次(high-level)的基于python的web爬虫构架。
用来下载、并解析web页面,其parse->yielditem->pipeline流程是所有爬虫的
固有模式。构造形式主要分
spider.pypipeline.pyitem.pydecorator.pymiddlewares.pysetting.py

### Scrapy的优缺点
- 优点：
    - scrapy是异步的
    - 采取可读性更强的xpath代替正则
    - 强大的统计和log系统
    - 同时在不同的url上爬行
    - 支持shell方式，方便独立调试
    - 写middleware,方便写一些统一的过滤器
    - 通过管道的方式存入数据库

- 缺点：
    - 基于python的爬虫框架，扩展性比较差
    - 基于twisted框架，运行中的exception是不会干掉reactor，并且异步框架出错后
    - 是不会停掉其他任务的，数据出错后难以察觉。

### scrapy和request
- scrapy是封装起来的框架，他包含了下载器，解析器，日志及异常处理
- scrapy基于多线程，twisted的方式处理，对于固定单个网站的爬取开发，有优势
- scrapy对于多网站爬取100个网站，并发及分布式处理方面，不够灵活，不便调整与括展
- request是一个HTTP库，它只是用来进行请求
- request请求，下载，解析全部自己处理，灵活性更高
- request高并发与分布式部署也非常灵活，对于功能可以更好实现.

### 常用的`反爬虫`及`反反爬虫`措施

分类|措施|反反
-|-|-
**判断用户身份** | User-Agent<br> Cookies<br> Refer<br> 验证码<br> | 设置headers<br> 4种破解验证码方法
**分析用户行为** | 并发量过大<br> 请求过于频繁<br> 在线活动时间过长<br> 访问到隐藏资源<br> | 降低并发数<br> 加入随机延时<br> 模拟人作息<br> 分析隐藏资源(hidden属性/text为空/超出窗口范围)
**动态加载数据** | AJAX<br> JavaScript | 浏览器抓包，获取JSON数据<br> JS逆向解析(可能有多次跳转)

### scrapy框架运行的机制

![scrapy](http://p4emt3ysm.bkt.clouddn.com/scrapy.png)

- 1) `Spiders`: 将`start_urls`里的地址封装成请求，并提交给引擎
- 2) `Spider Middlewares`: 请求去重
- 3) `Engine`: 将请求转发给调度器入队
- 4) `Scheduler`: 将请求队列中的请求按照优先级排序，并取出一定数量的请求给引擎
- 5) `Engine`: 将请求转发给下载器
- 6) `Downloader Middlewares`: 设置headers等自定义请求
- 7) `Downloader`: 获取请求对应的响应资源，并将响应给引擎
- 8) `Engine`: 将响应转发给爬虫
- 9) `Spiders`: 调用`parse`方法解析响应，得到字段数据`Items`和新`URL`并封装成响应`Response`，然后将`Items`和`Response`提交给引擎
- 10) `Engine`: 将字段数据`Items`转发给管道，将响应`Response`转发给调度器
- 11) `Pipeline`: 将数据保存到文件或者数据库
- 12) `Scheduler`: 检查请求队列中是否有请求，如果有重复执行上述步骤，如果没有程序结束

### scrapy和scrapy-redis的区别？为什么选择redis数据库
- 区别：
    - scrapy是一个Python爬虫框架，爬取效率极高，具有高度定制性，但是**不支持分布式**
    - scrapy-redis一套基于redis数据库、运行在scrapy框架之上的组件，可以让scrapy**支持分布式策略**
    - Slaver端共享Master端redis数据库里的item队列、请求队列和请求指纹集合

- 选择Redis原因：
    - redis支持主从同步
    - 数据都是缓存在内存中
    - 因此基于redis的分布式爬虫，对请求和高频读取效率非常高

### 实现模拟登录的方式有哪些
- 设置cookies
    - 使用一个具有登录状态的cookie，结合请求报头一起发送
    - 可以直接发送get请求，访问登录后才能访问的页面

- 利用session会话
    - 先发送登录界面的get请求，在登录页面HTML里获取登录需要的数据（如果需要的话），
    - 然后结合账户密码，再发送post请求，即可登录成功
    - 然后根据获取的cookie信息，继续访问之后的页面。

### 简单介绍下scrapy的异步处理
- scrapy框架的异步机制是基于twisted异步网络框架处理的
- 在settings.py文件里可以设置具体的并发量数值（默认是并发量16）

### Phontomjs相关

- 主程序退出后，selenium 不保证 phantomJS 也成功退出，最好手动关闭phantomJS 进程。（有可能会导致多个 phantomJS 进程运行，占用内存）
- WebDriverWait 虽然可能会减少延时，但是目前存在 bug（各种报错），这种情况可以采用 sleep。
- phantomJS 爬数据比较慢，可以选择多线程。如果运行的时候发现有的可以运行，有的不能，可以尝试将 phantomJS 改成 Chrome。

### scrapy-redis 去重原理

- 可见scrapy_redis是利用set数据结构来去重的，去重的对象是request的fingerprint
- （其实就是用hashlib .sha1()对request 对象的某些字段信息进行压缩）。
- request对象加密压缩后的一个字符串（40个字符，0~f）

### 怎么设置深度爬取

- 通过在`settings.py`中设置`DEPTH_LIMIT`的值可以限制爬取深度
- 这个深度是与`start_urls`中定义`url`的相对值。也就是相对`url`的深度。
- 若定义`url`为http://www.domz.com/game/, `DEPTH_LIMIT`=1那么限制爬取的只能是此`url`下一级的网页。
- 深度大于设置值的将被忽视。

### 代理ip里的“透明” “匿名” “高匿” 分别指什么

- `透明代理`
    - 传送的是真实的 IP
    - 是客户端根本不需要知道有代理服务器的存在
    - 你要想隐藏的话，不要用这个

- `普通匿名代理`
    - 隐藏客户机的真实IP
    - 会改变我们的请求信息
    - 服务器端知道我们使用了代理
    - 当然某些能够侦测ip的网页仍然可以查到你的ip

- `高匿名代理`
    - 隐藏客户机的真实IP
    - 不改变我们的请求信息
    - 服务器端不知道我们使用了代理

### `requests` 返回的 `context` 和 `text` 的区别

- `resp.text` 返回的是 `Unicode` 型的数据。
- `resp.content` 返回的是 `bytes` 型(二进制)的数据