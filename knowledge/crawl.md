# 爬虫

### 1. 说一下爬虫程序执行的流程（框架和三方库均可）


### 2. 爬虫在向数据库存数据开始和结束都会发一条消息，是scrapy哪个模块实现的
- Item Pipeline scrapy的信号处理使用的是 dispatch模块

### 3. 爬取下来的数据如何去重，说一下具体的算法依据
- 通过MD5生成电子指纹来判断页面是否改变
- `nutch` 去重。
    - nutch 中digest 是对采集的每一个网页内容的 32位哈希值，如果两个网页内容完全一样，它们的 digest 值肯定会 一样。

### 4. 写爬虫是用`多进程`还是`多线程`更好 
- IO 密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率
- 单线程下有IO操作会进行IO 等待，造成不必要的时间浪费
- 而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU 的资源，从而能提升程序执行效率
- 在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程。

### 5. 说一下numpy和pandas的区别 分别的应用场景
- `Numpy`是`Scipy`的扩展包，纯数学。
- `Pandas` 以矩阵为基础的数学计算模块。
    - 提供了一套名为DataFrame的数据结构，比较契合统计分析中的表结构
    - 提供了计算接口，可用`Numpy`或其它方式进行计算。

### 6. 验证码如何处理
- Scrapy自带处理验证码
- 获取到验证码图片的url，调用打码平台处理验证码

### 7. 微信公众号数据如何抓取
- sogou 微信搜索数据

### 8. 动态的股票信息如何抓取

- 股票数据的获取目前有如下两种方法可以获取:
    - http/JavaScript 接口取数据
    - web-service 接口
    - Sina 股票数据接口

- 以贵州茅台（股票代码：600519）为例，如果要获取它的最新行情，只需访问新浪的股票数据
    - ``https://hq.sinajs.cn/list=sh600519``
    - 返回的数据：

    > ``var hq_str_sh600519="贵州茅台,738.000,741.970,736.410,743.560,730.000,736.100,736.590,2599190,1912613830.000,300,736.100,100,736.060,100,736.050,500,736.000,200,735.900,200,736.590,900,736.600,100,736.670,100,736.760,400,736.770,2018-03-02,15:00:00,00";``

### 9. 爬虫部署
- 利用`scrapyd`进行爬虫部署

    > [scrapyd部署总结](http://blog.csdn.net/xiaoquantouer/article/details/53164306)

### 10. scrapy去重

- 数据量不大时，可以直接放在内存里面进行去重，python可以使用set()进行去重
- 数据需要持久化时可以使用redis的set数据结构。
- 当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成16/32/40个字符，再使用上面两种方法去重；
- 当数据量达到亿（甚至十亿、百亿）数量级时，内存有限，必须用“位”来去重，才能够满足需求。

> 然而Bloomfilter运行在一台机器的内存上，不方便持久化（机器down掉就什么都没啦），也不方便分布式爬虫的统一去重。
Bloomfilter就是将去重对象映射到几个内存“位”，通过几个位的0/1值来判断一个对象是否已经存在。
如果可以在Redis上申请内存进行Bloomfilter，以上两个问题就都能解决了。

> simhash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字，
然后判断重复只需要判断他们的特征字的距离是不是小于n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。

### 11. 分布式有哪些方案，哪一种最好
- `celery`、`beanstalk`，`gearman`

- 个人认为 `gearman` 比较好。原因主要有以下几点：
    - 1）技术类型简单，维护成本低。
    - 2）简单至上。能满足当前的技术需求即可 (分布式任务处理、异步同步任务同时支持、任务队列的持久化、维护部署简单)。
    - 3）有成熟的使用案例。instagram 就是使用的 gearman 来完成图片的处理的相关任务，有成功的经验，我们当然应该借鉴。

### 12. POST和GET区别

- 1、数据的位置
    - GET请求，请求的数据会附加在URL之后，以 `?` 分割URL和传输数据，多个参数用 `&` 连接。
        - URL的编码格式采用的是ASCII编码，而不是unicode，即是说所有的非ASCII字符都要编码之后再传输。
    - POST请求：POST请求会把请求的数据放置在HTTP请求包的包体中。上面的item=bandsaw就是实际的传输数据。
    - 因此，GET请求的数据会暴露在地址栏中，而POST请求则不会。

- 2、传输数据的大小
    - 在HTTP规范中，没有对URL的长度和传输的数据大小进行限制。
    - 但是在实际开发过程中，对于GET，特定的浏览器和服务器对URL的长度有限制。
    - 因此，在使用GET请求时，传输数据会受到URL长度的限制。
    - 对于POST，由于不是URL传值，理论上是不会受限制的，但是实际上各个服务器会规定对POST提交数据大小进行限制，Apache、IIS都有各自的配置。
- 3、安全性
    - POST的安全性比GET的高。

        > 这里的安全是指真正的安全，而不同于安全方法中的安全，那种安全仅仅是不修改服务器的数据。
    
    - 比如，在进行登录操作，通过GET请求，用户名和密码都会暴露再URL上，因为登录页面有可能被浏览器缓存以及其他人查看浏览器的历史记录的原因，此时的用户名和密码就很容易被他人拿到了。
    - 除此之外，GET请求提交的数据还可能会造成`Cross-site request frogery`攻击。

### 13. 三次握手和四次挥手

- **三次握手**
建立连接的过程是利用客户端服务器模式，假设主机A为客户端，主机B为服务器端。
    - （1）TCP的三次握手过程：主机A向B发送连接请求；主机B对收到的主机A的报文段进行确认；主机A再次对主机B的确认进行确认。
    - （2）采用三次握手是为了防止失效的连接请求报文段突然又传送到主机B，因而产生错误。
失效的连接请求报文段是指：主机A发出的连接请求没有收到主机B的确认，于是经过一段时间后，主机A又重新向主机B发送连接请求，且建立成功，顺序完成数据传输。
考虑这样一种特殊情况，主机A第一次发送的连接请求并没有丢失，而是因为网络节点导致延迟达到主机B，主机B以为是主机A又发起的新连接，于是主机B同意连接，并向主机A发回确认，但是此时主机A根本不会理会，主机B就一直在等待主机A发送数据，导致主机B的资源浪费。
    - （3）采用两次握手不行，原因就是上面说的失效的连接请求的特殊情况，因此采用三次握手刚刚好，两次可能出现失效，四次甚至更多次则没必要，反而复杂了

- **四次挥手**
    - 先由客户端向服务器端发送一个FIN，请求关闭数据传输。
    - 当服务器接收到客户端的FIN时，向客户端发送一个ACK，其中ack的值等于FIN+SEQ
    - 然后服务器向客户端发送一个FIN，告诉客户端应用程序关闭。
    - 当客户端收到服务器端的FIN是，回复一个ACK给服务器端。其中ack的值等于FIN+SEQ

- 名词解释
    - SYN: Synchronize Sequence Numbers 同步序列编号
    - ACK: Acknowledgement Number 确认编号
    - FIN: Finish 结束标志

![三次握手&四次挥手图解](http://p4emt3ysm.bkt.clouddn.com/tcp.png)

### 14. 多线程有哪些模块
- 在Python中可使用的多线程模块主要有两个，`thread` 和 `threading` 模块。

### 15. 谈一谈你对Selenium和PhantomJS了解
- Selenium是一个Web的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发
生。Selenium自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们
可以用一个叫PhantomJS的工具代替真实的浏览器。
Selenium库里有个叫WebDriver的API。WebDriver有点儿像可以加载网站的浏览器，但是它也可以像XPath或者其他Selector对象一样用来查找页
面元素，与页面上的元素进行交互(发送文本、点击等)，以及执行其它动作来运行网络爬虫。

- PhantomJS是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的JavaScript，
因为不会展示图形界面，所以运行起来比完整的浏览器要高效。

- 如果我们把Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了，
这个爬虫可以处理JavaScript、Cookies、headers，以及任何我们真实用户需要做的事情。

### 16. 常见的反爬虫和应对方法
- **1).通过Headers反爬虫**
    - 从用户请求的Headers反爬虫是最常见的反爬虫策略，在爬虫中修改或者添加Headers就能很好的绕过。
    - 很多网站都会对Headers的**User-Agent**进行检测 -> 将浏览器的User-Agent复制到爬虫的Headers中
    - 还有一部分网站会对**Refer**进行检测（一些资源网站的防盗链就是检测Refer)。 -> 将Refer值修改为目标网站域名

- **2).基于用户行为反爬虫**
    - **同一IP** 短时间内多次访问同一页面(大部分网站)
        - 使用IP代理
        - 专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。可以每请求几次更换一个ip
    - **同一账户** 短时间内多次进行相同操作
        - 可以在每次请求后随机间隔几秒再进行下一次请求
        - 有些有逻辑漏洞的网站，可以通过“请求-退出登录-重新登录-继续请求”，来绕过*同一账号短时间内不能多次进行相同请求*的限制。
- **3).动态页面的反爬虫**
    - 还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。
    - 首先用Fiddler对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests模拟ajax请求，对响应的json进行分析得到需要的数据。
    - 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。
    - 这种情况下就用selenium+phantomJS，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本

### 17. 动态加载又对及时性要求很高怎么处理
- Selenium+Phantomjs
- 尽量不使用sleep而使用WebDriverWait

### 18. 分布式爬虫主要解决什么问题
- ip
- 带宽
- cpu
- io

### 19. 什么是URL
- URL，即统一资源定位符，也就是我们说的网址
- 统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址
- 互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。


### 20. python爬虫有哪些常用技术
- Scrapy
- requests
- XPath
- BeautifulSoup
- urllib
- urllib2

### 21. 简单说一下你对scrapy的了解
scrapy是一个快速(fast)、高层次(high-level)的基于python的web爬虫构架。
用来下载、并解析web页面,其parse->yielditem->pipeline流程是所有爬虫的
固有模式。构造形式主要分
spider.pypipeline.pyitem.pydecorator.pymiddlewares.pysetting.py

### 22. Scrapy的优缺点
- 优点：
    - scrapy是异步的
    - 采取可读性更强的xpath代替正则
    - 强大的统计和log系统
    - 同时在不同的url上爬行
    - 支持shell方式，方便独立调试
    - 写middleware,方便写一些统一的过滤器
    - 通过管道的方式存入数据库

- 缺点：
    - 基于python的爬虫框架，扩展性比较差
    - 基于twisted框架，运行中的exception是不会干掉reactor，并且异步框架出错后
    - 是不会停掉其他任务的，数据出错后难以察觉。

### 23. scrapy和request
- scrapy是封装起来的框架，他包含了下载器，解析器，日志及异常处理
- scrapy基于多线程，twisted的方式处理，对于固定单个网站的爬取开发，有优势
- scrapy对于多网站爬取100个网站，并发及分布式处理方面，不够灵活，不便调整与括展
- request是一个HTTP库，它只是用来进行请求
- request请求，下载，解析全部自己处理，灵活性更高
- request高并发与分布式部署也非常灵活，对于功能可以更好实现.

### 24. 网络传输层

层|协议
-|-
应用层 | http ftp dns nfs
传输层 | tcp udp
网络层 | ip icmp igmp
链路层 | data link
物理层 | media

### 25. 设置ip和掩码

```bash
ifconfig eth0 192.168.6.8 netmask 255.255.255.0
```

### 26. 设置网关

```bash
route add default gw 192.168.6.1
```

### 27. `2MSL`

- `2MSL`即两倍的`MSL`，TCP的`TIME_WAIT`状态也称为`2MSL`等待状态，
- 当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了`TIME_WAIT`状态
- 必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到
- 那么对方在超时后将重发第三次握手的`FIN`包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包
- 在`TIME_WAIT`状态时两端的端口不能使用，要等到`2MSL`时间结束才可继续使用。
- 当连接处于2MSL等待阶段时任何迟到的报文段都将被丢弃。
- 在实际应用中可以通过设置`SO_REUSEADDR`选项达到不必等待2MSL时间结束再使用此端口。

### 28. `TTL`，`MSL`，`RTT`

- `TTL`：**生存时间** (`timetolive`)
    - 这个生存时间是由源主机设置初始值但不是生存的具体时间，而是存储了一个ip数据报可以经过的最大路由数
    - 每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃
    - 同时发送`ICMP`报文通知源主机。`RFC793`中规定`MSL`为2分钟，实际应用中常用的是30秒，1分钟和2分钟等
    - `TTL`与`MSL`是有关系的但不是简单的相等的关系，`MSL`要大于等于`TTL`

- `MSL`：**报文最大生存时间** (`Maximum Segment Lifetime`)
    - 它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。

- `RTT`：**客户到服务器往返所花时间** (`round-trip time`)
    - TCP含有动态估算RTT的算法。
    - TCP还持续估算一个给定连接的RTT，这是因为RTT受网络传输拥塞程序的变化而变化。

### 29. 创建一个简单tcp服务器需要的流程

No.|步骤|方法
-|-|-
1 | 创建套接字 | tcp_server_socket = socket(AF_INET, SOCK_STREAM)
2 | 设置服务器地址 | server_addr = ('127.0.0.1', 6666)
3 | 绑定地址 | tcp_server_socket.bind(server_addr)
4 | 监听 | tcp_server_socket.listen(128)
5 | 生成客户端套接字 | client_socket, client_addr = tcp_server_socket.accept()
6 | 接受内容 | recv_msg = client_socket.recv(1024).decode('gbk')
7 | 发送内容 | client_socket.send(send_msg.encode('gbk'))
8 | 关闭套接字 | client_socket.close() tcp_server_socket.close()

### 30. 常用的`反爬虫`及`反反爬虫`措施

分类|措施|反反
-|-|-
**判断用户身份** | User-Agent<br> Cookies<br> Refer<br> 验证码<br> | 设置headers<br> 4种破解验证码方法
**分析用户行为** | 并发量过大<br> 请求过于频繁<br> 在线活动时间过长<br> 访问到隐藏资源<br> | 降低并发数<br> 加入随机延时<br> 模拟人作息<br> 分析隐藏资源(hidden属性/text为空/超出窗口范围)
**动态加载数据** | AJAX<br> JavaScript | 浏览器抓包，获取JSON数据<br> JS逆向解析(可能有多次跳转)

### 31. 网络协议概述

- 应用层

    Abbr.   | Protocol                              | Description
    -|-|-
     FTP    | File Transfer Protocol                | 文件传输协议
     HTTP   | Hyper Text Transfer Protocol          | 超文本传输协议
     SMTP   | Simple Mail Transfer Protocol         | 简单邮件传输协议
     POP3   | Post Office Protocol                  | 邮局协议
     DNS    | Domain Name System                    | 域名系统

- 传输层

    Abbr.   | Protocol                              | Description
    -|-|-
     TCP    | Transmission Control Potocol          | 传输控制协议
     UDP    | User Datagram Potocol                 | 用户数据报协议

- 网络层

    Abbr.   | Protocol                              | Description
    -|-|-
     IP     | Internet Protocol                     | 网络协议
     ARP    | Adderss Resolution Protocol           | 地址解析协议
     ICMP   | Internet Control Message Protocol     | 控制报文协议
     HDLC   | High Data Link Control                | 高级数据链路控制

- 数据链路层

    Abbr.   | Protocol                              | Description
    -|-|-
     SLIP   | Serial Line Internet Protocol         | 串行线路网际协议
     PPP    |                                       | 点对点协议

- 物理层
    - 放大/再生若的信号，在两个电缆段之间复制每一个比特

### 32. HTTP/HTTPS的区别，应用场合
- HTTP和HTTPS的区别
    - 端口不一样,前者是80,后者是443
    - http是超文本传输协议，信息是明文传输，连接很简单，是无状态的
    - https协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全
    - https协议需要到ca申请证书，需要交费

- 应用场合
    - http:适合于对传输速度要求高，安全性要求不是很高，且需要快速开发的应用。如web应用，小的手机游戏等.
    - https:用于任何场景！

### 33. HTTPS优点和缺点
- 优点：
    - （1）使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器
    - （2）HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中被窃取、改变，确保数据的完整性
    - （3）HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本
    - （4）谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高”

- 缺点：
    - （1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电
    - （2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响
    - （3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用
    - （4）SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗
    - （5）HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行


### 34. HTTPS通信步骤

![https](http://p4emt3ysm.bkt.clouddn.com/https%E9%80%9A%E4%BF%A1%E8%BF%87%E7%A8%8B.png)

- （1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。
- （2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。
- （3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。
- （4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。
- （5）Web服务器利用自己的私钥解密出会话密钥。
- （6）Web服务器利用会话密钥加密与客户端之间的通信。


### 35. 国内外有哪些机构可以申请HTTPS证书

- 国内：
    - 沃通(WoSign) 中国人民银行联合12家银行建立的金融CFCA
    - 中国电信认证中心（CTCA） 海关认证中心（SCCA）
    - 国家外贸部EDI中心建立的国富安CA安全认证中心
    - SHECA（上海CA）为首的UCA协卡认证体系

- 国外：
    - StartSSL
    - GlobalSign
    - GoDaddy
    - Symantec

### 36. get和post请求有什么区别，分别应该在什么场合下

- **get**: 从指定的服务器中获取数据
    - GET请求能够被缓存
    - GET请求会保存在浏览器的浏览记录中
    - 以GET请求的URL能够保存为浏览器书签
    - GET请求有长度限制
    - GET请求主要用以获取数据

- **post**: 向指定的服务器中发送数据
    - POST请求不能被缓存下来
    - POST请求不会保存在浏览器浏览记录中
    - 以POST请求的URL无法保存为浏览器书签
    - POST请求没有长度限制
    - POST请求会把请求的数据放置在HTTP请求包的包体中
    - POST的安全性比GET的高.可能修改变服务器上的资源的请求.

- **应用场合**：
    - post：
        - 要传送的数据不是采用7位的ASCII编码
        - 请求的结果有持续性的副作用（数据库内添加新的数据行）
        - 如果使用GET方法，表单上收集的数据可能让URL过长
    - get：
        - 请求是为了查找资源，HTML表单数据仅用来帮助搜索
        - 请求结果无持续性的副作用
        - 收集的数据及HTML表单内的输入字段名称的总长不超过1024个字符

### 37. HTTP请求会有哪些信息发送到后台服务器
- 请求行
    - POST /demo/login HTTP/1.1
- 请求头
- 请求体
    - username=xxxx&password=1234

### 38. scrapy框架运行的机制

![scrapy](http://p4emt3ysm.bkt.clouddn.com/scrapy.png)

- 1) `Spiders`: 将`start_urls`里的地址封装成请求，并提交给引擎
- 2) `Spider Middlewares`: 请求去重
- 3) `Engine`: 将请求转发给调度器入队
- 4) `Scheduler`: 将请求队列中的请求按照优先级排序，并取出一定数量的请求给引擎
- 5) `Engine`: 将请求转发给下载器
- 6) `Downloader Middlewares`: 设置headers等自定义请求
- 7) `Downloader`: 获取请求对应的响应资源，并将响应给引擎
- 8) `Engine`: 将响应转发给爬虫
- 9) `Spiders`: 调用`parse`方法解析响应，得到字段数据`Items`和新`URL`并封装成响应`Response`，然后将`Items`和`Response`提交给引擎
- 10) `Engine`: 将字段数据`Items`转发给管道，将响应`Response`转发给调度器
- 11) `Pipeline`: 将数据保存到文件或者数据库
- 12) `Scheduler`: 检查请求队列中是否有请求，如果有重复执行上述步骤，如果没有程序结束

### 39. scrapy和scrapy-redis的区别？为什么选择redis数据库
- 区别：
    - scrapy是一个Python爬虫框架，爬取效率极高，具有高度定制性，但是**不支持分布式**
    - scrapy-redis一套基于redis数据库、运行在scrapy框架之上的组件，可以让scrapy**支持分布式策略**
    - Slaver端共享Master端redis数据库里的item队列、请求队列和请求指纹集合

- 选择Redis原因：
    - redis支持主从同步
    - 数据都是缓存在内存中
    - 因此基于redis的分布式爬虫，对请求和高频读取效率非常高

### 40. 实现模拟登录的方式有哪些
- 设置cookies
    - 使用一个具有登录状态的cookie，结合请求报头一起发送
    - 可以直接发送get请求，访问登录后才能访问的页面

- 利用session会话
    - 先发送登录界面的get请求，在登录页面HTML里获取登录需要的数据（如果需要的话），
    - 然后结合账户密码，再发送post请求，即可登录成功
    - 然后根据获取的cookie信息，继续访问之后的页面。

### 41. 简单介绍下scrapy的异步处理
- scrapy框架的异步机制是基于twisted异步网络框架处理的
- 在settings.py文件里可以设置具体的并发量数值（默认是并发量16）
