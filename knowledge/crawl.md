# 爬虫

### 1. 说一下爬虫程序执行的流程（框架和三方库均可）


### 2. 爬虫在向数据库存数据开始和结束都会发一条消息，是scrapy哪个模块实现的
- Item Pipeline scrapy的信号处理使用的是 dispatch模块

### 3. 爬取下来的数据如何去重，说一下具体的算法依据
- 通过MD5生成电子指纹来判断页面是否改变
- `nutch` 去重。
	- nutch 中digest 是对采集的每一个网页内容的 32位哈希值，如果两个网页内容完全一样，它们的 digest 值肯定会 一样。

### 4. 写爬虫是用`多进程`还是`多线程`更好 
- IO 密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率
- 单线程下有IO操作会进行IO 等待，造成不必要的时间浪费
- 而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU 的资源，从而能提升程序执行效率
- 在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程。

### 5. 说一下numpy和pandas的区别 分别的应用场景
- `Numpy`是`Scipy`的扩展包，纯数学。
- `Pandas` 以矩阵为基础的数学计算模块。
	- 提供了一套名为DataFrame的数据结构，比较契合统计分析中的表结构
	- 提供了计算接口，可用`Numpy`或其它方式进行计算。

### 6. 验证码如何处理
- Scrapy自带处理验证码
- 获取到验证码图片的url，调用打码平台处理验证码

### 7. 微信公众号数据如何抓取
- sogou 微信搜索数据

### 8. 动态的股票信息如何抓取

- 股票数据的获取目前有如下两种方法可以获取:
	- http/JavaScript 接口取数据
	- web-service 接口
	- Sina 股票数据接口

- 以贵州茅台（股票代码：600519）为例，如果要获取它的最新行情，只需访问新浪的股票数据
	- ``https://hq.sinajs.cn/list=sh600519``
	- 返回的数据：

	> ``var hq_str_sh600519="贵州茅台,738.000,741.970,736.410,743.560,730.000,736.100,736.590,2599190,1912613830.000,300,736.100,100,736.060,100,736.050,500,736.000,200,735.900,200,736.590,900,736.600,100,736.670,100,736.760,400,736.770,2018-03-02,15:00:00,00";``

### 9. 爬虫部署
- 利用`scrapyd`进行爬虫部署

	> [scrapyd部署总结](http://blog.csdn.net/xiaoquantouer/article/details/53164306)

### 10. scrapy去重

- 数据量不大时，可以直接放在内存里面进行去重，python可以使用set()进行去重
- 数据需要持久化时可以使用redis的set数据结构。
- 当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成16/32/40个字符，再使用上面两种方法去重；
- 当数据量达到亿（甚至十亿、百亿）数量级时，内存有限，必须用“位”来去重，才能够满足需求。

> 然而Bloomfilter运行在一台机器的内存上，不方便持久化（机器down掉就什么都没啦），也不方便分布式爬虫的统一去重。
Bloomfilter就是将去重对象映射到几个内存“位”，通过几个位的0/1值来判断一个对象是否已经存在。
如果可以在Redis上申请内存进行Bloomfilter，以上两个问题就都能解决了。

> simhash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字，
然后判断重复只需要判断他们的特征字的距离是不是小于n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。

### 11. 分布式有哪些方案，哪一种最好
- `celery`、`beanstalk`，`gearman`

- 个人认为 `gearman` 比较好。原因主要有以下几点：
	- 1）技术类型简单，维护成本低。
	- 2）简单至上。能满足当前的技术需求即可 (分布式任务处理、异步同步任务同时支持、任务队列的持久化、维护部署简单)。
	- 3）有成熟的使用案例。instagram 就是使用的 gearman 来完成图片的处理的相关任务，有成功的经验，我们当然应该借鉴。

### 12. POST和GET区别

- 1、数据的位置
	- GET请求，请求的数据会附加在URL之后，以 `?` 分割URL和传输数据，多个参数用 `&` 连接。
		- URL的编码格式采用的是ASCII编码，而不是unicode，即是说所有的非ASCII字符都要编码之后再传输。
	- POST请求：POST请求会把请求的数据放置在HTTP请求包的包体中。上面的item=bandsaw就是实际的传输数据。
	- 因此，GET请求的数据会暴露在地址栏中，而POST请求则不会。

- 2、传输数据的大小
	- 在HTTP规范中，没有对URL的长度和传输的数据大小进行限制。
	- 但是在实际开发过程中，对于GET，特定的浏览器和服务器对URL的长度有限制。
	- 因此，在使用GET请求时，传输数据会受到URL长度的限制。
	- 对于POST，由于不是URL传值，理论上是不会受限制的，但是实际上各个服务器会规定对POST提交数据大小进行限制，Apache、IIS都有各自的配置。
- 3、安全性
	- POST的安全性比GET的高。

		> 这里的安全是指真正的安全，而不同于安全方法中的安全，那种安全仅仅是不修改服务器的数据。
	
	- 比如，在进行登录操作，通过GET请求，用户名和密码都会暴露再URL上，因为登录页面有可能被浏览器缓存以及其他人查看浏览器的历史记录的原因，此时的用户名和密码就很容易被他人拿到了。
	- 除此之外，GET请求提交的数据还可能会造成`Cross-site request frogery`攻击。

### 13. 为什么要三次握手和四次挥手

- **三次握手**
建立连接的过程是利用客户端服务器模式，假设主机A为客户端，主机B为服务器端。
	- （1）TCP的三次握手过程：主机A向B发送连接请求；主机B对收到的主机A的报文段进行确认；主机A再次对主机B的确认进行确认。
	- （2）采用三次握手是为了防止失效的连接请求报文段突然又传送到主机B，因而产生错误。
失效的连接请求报文段是指：主机A发出的连接请求没有收到主机B的确认，于是经过一段时间后，主机A又重新向主机B发送连接请求，且建立成功，顺序完成数据传输。
考虑这样一种特殊情况，主机A第一次发送的连接请求并没有丢失，而是因为网络节点导致延迟达到主机B，主机B以为是主机A又发起的新连接，于是主机B同意连接，并向主机A发回确认，但是此时主机A根本不会理会，主机B就一直在等待主机A发送数据，导致主机B的资源浪费。
	- （3）采用两次握手不行，原因就是上面说的失效的连接请求的特殊情况，因此采用三次握手刚刚好，两次可能出现失效，四次甚至更多次则没必要，反而复杂了

- **四次挥手**
	- 先由客户端向服务器端发送一个FIN，请求关闭数据传输。
	- 当服务器接收到客户端的FIN时，向客户端发送一个ACK，其中ack的值等于FIN+SEQ
	- 然后服务器向客户端发送一个FIN，告诉客户端应用程序关闭。
	- 当客户端收到服务器端的FIN是，回复一个ACK给服务器端。其中ack的值等于FIN+SEQ

- 为什么要4次挥手？
	- 确保数据能够完成传输

![三次握手&四次挥手图解](http://p4emt3ysm.bkt.clouddn.com/tcp.png)

### 14. 多线程有哪些模块
- 在Python中可使用的多线程模块主要有两个，`thread` 和 `threading` 模块。

### 15. 谈一谈你对Selenium和PhantomJS了解
- Selenium是一个Web的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发
生。Selenium自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们
可以用一个叫PhantomJS的工具代替真实的浏览器。
Selenium库里有个叫WebDriver的API。WebDriver有点儿像可以加载网站的浏览器，但是它也可以像XPath或者其他Selector对象一样用来查找页
面元素，与页面上的元素进行交互(发送文本、点击等)，以及执行其它动作来运行网络爬虫。

- PhantomJS是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的JavaScript，
因为不会展示图形界面，所以运行起来比完整的浏览器要高效。

- 如果我们把Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了，
这个爬虫可以处理JavaScript、Cookies、headers，以及任何我们真实用户需要做的事情。

### 16. 常见的反爬虫和应对方法
- **1).通过Headers反爬虫**
	- 从用户请求的Headers反爬虫是最常见的反爬虫策略，在爬虫中修改或者添加Headers就能很好的绕过。
	- 很多网站都会对Headers的**User-Agent**进行检测 -> 将浏览器的User-Agent复制到爬虫的Headers中
	- 还有一部分网站会对**Referer**进行检测（一些资源网站的防盗链就是检测Referer)。 -> 将Referer值修改为目标网站域名

- **2).基于用户行为反爬虫**
	- **同一IP** 短时间内多次访问同一页面(大部分网站)
		- 使用IP代理
		- 专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。可以每请求几次更换一个ip
	- **同一账户** 短时间内多次进行相同操作
		- 可以在每次请求后随机间隔几秒再进行下一次请求
		- 有些有逻辑漏洞的网站，可以通过“请求-退出登录-重新登录-继续请求”，来绕过*同一账号短时间内不能多次进行相同请求*的限制。
- **3).动态页面的反爬虫**
	- 还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。
	- 首先用Fiddler对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests模拟ajax请求，对响应的json进行分析得到需要的数据。
	- 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。
	- 这种情况下就用selenium+phantomJS，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本

### 17. 动态加载又对及时性要求很高怎么处理
- Selenium+Phantomjs
- 尽量不使用sleep而使用WebDriverWait

### 18. 分布式爬虫主要解决什么问题


### 19. 什么是URL


### 20. python爬虫有哪些常用技术


### 21. 简单说一下你对scrapy的了解


### 22. Scrapy的优缺点


### 23. scrapy和request


### 24. 网络传输层


### 25. 设置ip和掩码


### 26. 设置网关


### 27. 什么是2MSL


### 28. 创建一个简单tcp服务器需要的流程


### 29. TTL，MSL，RTT


### 30. 常用的反爬虫措施


### 31. 网络协议概述


### 32. 关于HTTP/HTTPS的区别，分别应该在什么场合下


### 33. HTTPS有什么优点和缺点


### 34. HTTPS是如何实现安全传输数据的


### 35. HTTPS安全证书是怎么来的，如何申请，国内和国外有哪些第三方机构提供安全证书认证。


### 36. get和post请求有什么区别，分别应该在什么场合下


### 37. HTTP请求会有哪些信息发送到后台服务器


### 38. 描述下scrapy框架运行的机制


### 39. scrapy和scrapy-redis有什么区别？为什么选择redis数据库


### 40. 实现模拟登录的方式有哪些


### 41. 简单介绍下scrapy的异步处理
